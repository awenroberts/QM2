{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/awenroberts/QM2/blob/main/assessed_question_W9_Distributions_and_Basic_Statistics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lads9rv6exJ-"
      },
      "source": [
        "# Difference in Differences\n",
        "\n",
        "## *Workshop 9*  [![Open In Colab](https://github.com/oballinger/QM2/blob/main/colab-badge.png?raw=1)](https://colab.research.google.com/github/oballinger/QM2/blob/main/notebooks/W08.%20Diff-in-Diff.ipynb)\n",
        "\n",
        "### Aims:\n",
        "\n",
        "This workshop builds on last week's material, replicating analysis in published academic research on the relationship between minimum wages and unemployment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KugRUBXRUqf"
      },
      "source": [
        "As always we'll start by importing the libraries I need"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ERdQ-QjexKB"
      },
      "outputs": [],
      "source": [
        "#!pip install linearmodels\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import plotly\n",
        "import plotly.express as px\n",
        "import warnings\n",
        "from statsmodels.formula.api import ols\n",
        "from statsmodels.iolib.summary2 import summary_col\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "\n",
        "from math import sqrt\n",
        "from numpy.random import seed\n",
        "from numpy.random import randn\n",
        "from numpy import mean\n",
        "from scipy.stats import sem\n",
        "import statistics\n",
        "import seaborn as sns\n",
        "from IPython.display import display, Math, Latex, display_latex\n",
        "import pylab\n",
        "\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set(font_scale=1.5)\n",
        "sns.set_style(\"white\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bce58KTyylbS"
      },
      "source": [
        "----------------------\n",
        "## Panel Regression\n",
        "\n",
        "[Surveys](https://www.pewresearch.org/politics/2012/04/17/with-voters-focused-on-economy-obama-lead-narrows/) indicate that \"**jobs**\" are consistently one of the most important issues among voters in U.S. presidential elections, and that **Republicans** are [typically perceived](https://thehill.com/homenews/campaign/3700047-republicans-hold-14-point-advantage-on-which-party-would-do-better-job-on-economy-poll/) as **better** in handling the economy than Democrats. An [article](https://www.nbcnews.com/news/us-news/where-did-trump-make-election-gains-unemployment-data-tells-surprising-n1247935) in NBC claims that \"analysis of unemployment and voting data found that the president’s share of the vote held steady or increased in each of the 20 counties with the highest rise in unemployment from September 2019 to September 2020. And his vote share improved by 1 percentage point or more in 70 of the 100 hardest-hit counties.\" Let's look into this.\n",
        "\n",
        "in 20 counties with the highest **rise in unemployment**\n",
        "- votes for president held steady/ increased.\n",
        "\n",
        "in 70/100 hardest hit counties\n",
        "- votes increased by 1% or more\n",
        "\n",
        "### Data Collection\n",
        "\n",
        "There are only 50 states in the U.S. but there are over **3000 counties**-- this allows us to increase our sample size and perform a more fine-grained analysis. This is particularly important if we're interested in investigating the **relationship between unemployment and voting behaviour**, because of the **urban-rural divide**. For example, with in the state of New York there are probably vast differences in social and economic factors relevant to voting behaviour between Manhattan and very rural areas; this variation is lost when we look at aggregate state-level resutls, but visible when we look at the county-level. As such, in addition to the datasets we've just imported, we're going to be downloading **county-level unemployment data** straight from the BLS using the loop below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRmcXCJvylbS"
      },
      "outputs": [],
      "source": [
        "counties=pd.read_csv('https://storage.googleapis.com/qm2/wk10/county_labor.csv', converters={'county_fips': str})\n",
        "counties.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PNUbnupylbS"
      },
      "source": [
        "Part of the cleaning process in the cell above involves the creation of a column called \"county_fips\"-- this stands for [Federal Information Processing System](https://transition.fcc.gov/oet/info/maps/census/fips/fips.txt#:~:text=FIPS%20codes%20are%20numbers%20which,to%20which%20the%20county%20belongs.). This is a code that uniquely identifies states and counties in the U.S. A two digit FIPS code identifies states (e.g. 01: Alabama, 02: Alaska, etc.) and a five digit fips code identifies counties (e.g. 010001: Atauga County, Alabama; 02068: Denali Borough, Alaska). Notice, the first two digits of the five-digit county FIPS code indicates the state. Boring, yes, but these codes are imperative in allowing us to join county- and state- level datasets from different sources quicky and easily. Imagine what a nightmare it would be to try to join them using the names of the counties, having to deal with capitalizations, punctuation, etc. Yikes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKpKdf75ylbS"
      },
      "source": [
        "### Maps\n",
        "\n",
        "Great-- we've now got clean, **county-level unemployment and population data spanning from 1990-2022 on an annual basis**. Lets make a map to explore the spatial distribution of unemployment across time in the U.S. In order to do that, we're going to need a spatial file that tells us the shapes of the counties; I've imported it as a variable called `county_polygons`. We're then going to create an  map using the [Plotly](https://plotly.com/python/) library, which is great for making pretty, interactive maps and plots. It will have a slider on the bottom that lets us view unemployment in different years. It's doing quite a bit under the hood so it will take some time to plot. Be patient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvxz7JuLylbS"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "!mkdir data\n",
        "!mkdir data/wk10/\n",
        "!curl https://storage.googleapis.com/qm2/wk10/geojson-counties-fips.json -o data/wk10/geojson-counties-fips.json\n",
        "\n",
        "county_polygons = json.load(open(\"data/wk10/geojson-counties-fips.json\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rf39g1xjylbS"
      },
      "outputs": [],
      "source": [
        "plot_sample=counties[counties['year']>2007] # subset the data to only include years after 2007 -- it would take too long to plot all of the data\n",
        "\n",
        "px.choropleth( # plot a choropleth map using the plotly express (px) library\n",
        "                plot_sample, # load the dataframe\n",
        "                locations='county_fips', # set the location column to the state code\n",
        "                geojson=county_polygons, # set the location mode to USA states (you could add your own custom geojson/spatial file here)\n",
        "                scope='usa', # set the scope to the USA, so that it only plots the states\n",
        "                color=\"unemployment\", # set the color of the states to correspond to the unemployment rate\n",
        "                animation_frame=plot_sample[\"year\"].astype(str), # set the animation frame to the date, creating a slider at the bottom of the map\n",
        "                color_continuous_scale=px.colors.sequential.Viridis, # set the color scale to Viridis, a commonly used color scale\n",
        "                range_color=[0, 10], # set the range of the color scale to 0-10\n",
        "                height=1000) # set the height of the map to 1000 pixels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAAyStNMylbT"
      },
      "source": [
        "This map is interactive-- meaning you can zoom in, pan around, and hover over it to get further information on the unemployment level in each county. You can also use the slider at the bottom to toggle between different years; if you move the slider from 2008 to 2009, you'll see lots of yellow suddenly appearing. A similar thing happens between 2019 and 2020. What's going on? Play around with this map for a second, and make note of spatial and temporal trends in unemployment.\n",
        "\n",
        "2008-2009: The Great Recession. Systemic failures in the financial system, collapse of housing market: impacts on construction and finance industries. credit crisis, collapse in aggragate demand. Gradual increase of unemployment.\n",
        "\n",
        "2019-2020: COVID-19 Pandemic.\n",
        "Widespread shutdown of businesses,triggered a sudden halt to many ecomonic activities. Unemployment exploded from Feb to April.\n",
        "\n",
        "Now we're going to **do the same thing for the elections data**, which I've taken the liberty of cleaning. Let's load it up as a dataframe called `elections`, and make another map in which we plot vote shares in various elections such that red shows republican support, and blue shows democratic support."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPwYf9JJylbT"
      },
      "outputs": [],
      "source": [
        "elections=pd.read_csv('https://storage.googleapis.com/qm2/wk10/elections.csv',converters={'county_fips': str})\n",
        "px.choropleth(\n",
        "                elections,\n",
        "                locations='county_fips',\n",
        "                geojson=county_polygons,\n",
        "                scope='usa',\n",
        "                color=\"r_votes\",\n",
        "                animation_frame=elections[\"year\"].astype(str),\n",
        "                color_continuous_scale=px.colors.diverging.balance,\n",
        "                range_color=[20, 80],\n",
        "                height=800)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "elections.head()"
      ],
      "metadata": {
        "id": "vlQ1_fH3SIVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2s1JJjAylbT"
      },
      "source": [
        "Explore the map above. What do you notice about republican vote share, particularly as it relates to the previous map of unemployment?\n",
        "\n",
        "More democratic votes after incidents of high unemployment spikes.\n",
        "\n",
        "Now we've got **two datasets**-- one on **unemployment** and another on **election results**. We want to **merge** them but CAREFUL: each row corresponds to the value of a variable $x$ in county $i$ and time $t$ (so, $x_{it}$); for example, the value in the first row of our dataset under the unemployment column would be $unemployment_{01001, 2000}$; i.e., the unemployment rate in Atauga County, Alabama (FIPS code 01001), in the year 2000. When our data has this structure ($x_{it}$), we call it **panel data**. It must be handled differently from **cross sectional data** ($x_i$), from merging to estimation.\n",
        "\n",
        "We can't just merge on $i$ or $t$, we need to merge on both. We can do so as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlDPhfxlylbT"
      },
      "outputs": [],
      "source": [
        "df_c=pd.merge(elections,counties, on=['county_fips','year'])\n",
        "df_c.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_vGoJSPylbT"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "OK. Our data is clean and ready for analysis. Because we're going to be investigating the relationship between unempoyment rates and republican voteshare via a **regression model**, we're going to need to follow the four steps of regression modeling from [last week](https://oballinger.github.io/QM2/notebooks/W09.%20Linear%20Regression.html).\n",
        "\n",
        "First, **formulate a research question** (complete with null and alternative hypothesis), and then follow these steps for our dataset, `df_c` (bonus points if you account for the influence of population).\n",
        "\n",
        "1. Summary Statistics\n",
        "    * Table of Summary Statistics\n",
        "2. Visualisation\n",
        "    * Exploratory Plots\n",
        "3. Assumptions\n",
        "    * A. Independence\n",
        "    * B. Heteroscedasticity: Regression plots + Q-Q plot\n",
        "    * C. Multicollinearity: VIF + Correlation Matrix\n",
        "4. Regression\n",
        "    * Regression Table\n",
        "\n",
        "For the moment, when you run the regression, ignore the fact that we have panel data and just run a regular regression of the form\n",
        "\n",
        "$$\\huge Y= \\beta_0 + \\beta_1X+\\epsilon $$\n",
        "\n",
        "### Accounting for Space and Time\n",
        "\n",
        "If you've done things correctly, you'll notice two things. First, the appears to be a **generally negative relationship between unemployment and republican voteshare**; in other words, places with higher unemployment tend to vote *against* republicans. Second, we've egregiously **violated the independence assumption**. We have repeat observations of the same individuals (counties) over time. As such, this result may be biased unless we account for space and time.\n",
        "\n",
        "As we saw in the lecture, **panel data** actually contains *two* sources of variation: differences *between* individuals (in this case, counties), and *within* individuials. So, a simple research question such as \"Does unemployment increase republican voteshare\" is actually two different questions:\n",
        "\n",
        "1. Does a higher level of unemployment lead to higher republican vote shares **between counties**?\n",
        "2. Does an *increase* in the unemployment rate over time lead to an *increase* in republican vote shares **within counties**?\n",
        "\n",
        "Neither is more important than the other, but we must be careful not to conflate them as they are very different questions. A straighforward way of answering the first question would be to get rid of the time dimension in our data by running a separate regression for each year:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Formulate a research question\n",
        "\n",
        "This analysis will be investigating the relationship between unemployment and voting behaviour. Spesifically, if unemployment is related to republican vote share.\n",
        "\n",
        "Question:\n",
        "\n"
      ],
      "metadata": {
        "id": "6qJAvWHpAAgo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Summary Statistics"
      ],
      "metadata": {
        "id": "px6Couvo_BZq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7925dc4d"
      },
      "source": [
        "df_c.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Convert data types\n",
        "df_c['year']=df_c['year'].astype(int)\n",
        "df_c['county_fips']=df_c['county_fips'].astype('object') # Corrected to 'object' for string-like FIPS codes\n",
        "df_c['state']=df_c['state'].astype('category') # Corrected to 'category' string\n",
        "df_c['county']=df_c['county'].astype('category') # Fixed typo from 'country' to 'county' and corrected to 'category' string\n",
        "df_c.info()"
      ],
      "metadata": {
        "id": "25Fapl5fiuRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate summary statistics, and round everything to 2 decimal degrees\n",
        "summary=df_c.describe().round(2)\n",
        "\n",
        "#.T transposes the table (rows become columns and vice versa)\n",
        "summary=summary.T\n",
        "summary"
      ],
      "metadata": {
        "id": "DhcdL6dr_rxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Visualisation\n",
        "\n",
        "- Exploratory Plots to visualise the distribution of categorical values.\n",
        "- Look at distribution of population and unemployment per state/ county?"
      ],
      "metadata": {
        "id": "sGTaElst_Esa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Aggregate population by state for bar plot\n",
        "state_population = df_c.groupby('state')['population'].sum().reset_index()\n",
        "\n",
        "sns.barplot(data=state_population, x='state', y='population') # Use barplot to show aggregated population\n",
        "\n",
        "plt.title('Total Population per State') # add a title\n",
        "plt.xlabel('State FIPS Code') # change x axis label to reflect FIPS codes\n",
        "\n",
        "# Adjust x-ticks: since 'state' is a FIPS int, let's just show a few for illustration or remove if there are too many\n",
        "# For better labeling, you'd ideally merge state names or map FIPS codes to names.\n",
        "# For now, let's just make sure the existing labels don't cause an error.\n",
        "# The original xticks line was problematic with 'state' as an int/category.\n",
        "# Let's try to map some FIPS to names or just remove the specific xticks if too many states.\n",
        "# For now, let's set simpler labels assuming the FIPS codes are somewhat sequential on the plot.\n",
        "plt.xticks(rotation=45, ha='right') # Rotate for better readability if many states\n",
        "plt.ylabel('Total Population (in thousands)')\n",
        "plt.show() # show the plot"
      ],
      "metadata": {
        "id": "ZAF8qlnn_rer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate unemployment by state for bar plot\n",
        "state_unemployment = df_c.groupby('state')['unemployment'].sum().reset_index()\n",
        "\n",
        "sns.barplot(data=state_unemployment, x='state', y='unemployment') # Use barplot to show aggregated unemplpoyment\n",
        "\n",
        "plt.title('Unemployment per State') # add a title\n",
        "plt.xlabel('State FIPS Code') # change x axis label to reflect FIPS codes\n",
        "plt.ylabel('Unemployment (% of population)')\n",
        "plt.show() # show the plot"
      ],
      "metadata": {
        "id": "rzzt3cJImL6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Assumptions\n",
        "\n",
        "- A. Independence\n",
        "- B. Heteroscedasticity: Regression plots + Q-Q plot\n",
        "- C. Multicollinearity: VIF + Correlation Matrix\n"
      ],
      "metadata": {
        "id": "iUvw8PC2_Rro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "INDEPENDENCE: Linear regression assumes that measurements for each sample subject are in no way influenced by or related to the measurements of other subjects.\n",
        "- this dataset violates the independence assumption because the data is measured over the years. but we will ignore this for now.\n"
      ],
      "metadata": {
        "id": "UJBJdxBwnQ28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "HOMOSCEDACITY: Linear regression assumes that the variance of residuals is the same for any value of x, and that residuals are normally distributed with a mean of 0.\n",
        "\n",
        "- explore this by visually inspecting a regression plot. we are interested in the republican vote share vs unemployment levels; we can plot these variables against eachother using sns.jointplot()."
      ],
      "metadata": {
        "id": "Ud0ubgSMnhD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.jointplot(data=df_c, # plot a scatterplot with a regression line and two histograms\n",
        "                x='unemployment', # set the x axis to be unemployment\n",
        "                y='r_votes', # set the y axis to be the republican vote share\n",
        "                kind=\"reg\",  # set the kind of plot to be a regression plot\n",
        "                scatter_kws=dict(alpha=0.1), # set the transparency of the points to be 0.1 (10%)\n",
        "                line_kws=dict(color='red'), # set the color of the regression line to red\n",
        "                height=10) # set the height of the plot to be 10 inches\n",
        "\n",
        "plt.xlabel('Unemployment') # add a label to the x axis\n",
        "plt.ylabel('Republican Vote share') # add a label to the y axis"
      ],
      "metadata": {
        "id": "bxp6CU9VoARB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- republican vote share is relatively normally distributed\n",
        "- unemployment is heavily right skewed\n",
        "- this results in heteroscedascity: regression fits the lower values better than the higher values."
      ],
      "metadata": {
        "id": "PIZaXK-Nojar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN A Q-Q PLOT\n",
        "\n",
        "model = ols('r_votes ~  unemployment', data=df_c).fit()  # fit a model\n",
        "residuals = model.resid # get the residuals\n",
        "\n",
        "# make the figure wider\n",
        "plt.rcParams[\"figure.figsize\"] = [20, 10]\n",
        "\n",
        "f, axes = plt.subplots(1, 2)\n",
        "sns.histplot(residuals, kde=True, ax=axes[0]) # plot the residuals\n",
        "axes[0].set_title('Histogram of Residuals') # add a title\n",
        "\n",
        "sm.qqplot(residuals, line='45', fit=True,  ax=axes[1]) # plot the residuals\n",
        "axes[1].set_title('Q-Q Plot') # add a title\n",
        "\n",
        "plt.show() # show the plot"
      ],
      "metadata": {
        "id": "xQ8URM7dpUlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Regression\n",
        "- Regression Table"
      ],
      "metadata": {
        "id": "TigGK4H7_UBN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "REGRESSION:\n",
        "The OLS (ordinary least squares) regressoin seeks to find a straight line that best describes the realtionship between two variables:\n",
        "$$y= \\beta_0 + \\beta_1x+\\epsilon $$\n",
        "\n",
        "in our case:\n",
        "\n",
        "$$Republican\\ Voteshare= \\beta_0 + \\beta_1 \\times Unemployment+\\epsilon $$\n"
      ],
      "metadata": {
        "id": "WsccSQ7rqPKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.formula.api import ols\n",
        "from statsmodels.iolib.summary2 import summary_col\n",
        "\n",
        "#run the ols regression\n",
        "model= ols('r_votes ~  unemployment', data=df_c).fit() # fit the model\n",
        "print(model.summary()) # print the summary"
      ],
      "metadata": {
        "id": "5pzj2OiD--nM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation of R-squared:** 0.066\n",
        "only 6.6% of the variation in republican vote share can be explained by unemployment.\n",
        "\n",
        "Interpretation of coefficients:\n",
        "**Intercept**: 71.34\n",
        "When unemployment = 0, the republican vote share is predicted to be 71%. However, since unemployment never equals 0 in real data, this number is not substantively meaningful.\n",
        "**unemployment:** -1.60\n",
        "With each 1% increase in unemployment, republican vote share decreases by 1.61 percentage points, on average. This is statistically significant at p<0.001.\n",
        "\n"
      ],
      "metadata": {
        "id": "AGsOE4HhrXmq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cont"
      ],
      "metadata": {
        "id": "yoeKuztg_ko4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SxElNktylbT"
      },
      "outputs": [],
      "source": [
        "models=[] # create empty list to store the models\n",
        "names=[] # create empty list to store the names of the models\n",
        "years=df_c['year'].unique()\n",
        "\n",
        "for year in years: # loop through years from 2000 to 2020 in increments of 4\n",
        "    election=df_c[df_c['year']==year] # subset the data to only include the year of interest\n",
        "    model= ols('r_votes ~ unemployment + population', data=election).fit() # run a regression of the republican vote share on the unemployment rate\n",
        "    models.append(model) # append the model to the list of models\n",
        "    names.append(str(year)) # append the name of the model to the list of names\n",
        "\n",
        "table=summary_col( # create a regression table\n",
        "    models, # pass the models to the summary_col function\n",
        "    stars=True, # add stars denoting the p-values of the coefficient to the table; * p<0.05, ** p<0.01, *** p<0.001\n",
        "    float_format='%0.3f', # set the decimal places to 3\n",
        "    model_names=names, # set the names of the model\n",
        "    info_dict = {\"N\":lambda x: \"{0:d}\".format(int(x.nobs))}) # add the number of observations to the table\n",
        "\n",
        "print(table) # print the table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_D7qTW3ylbT"
      },
      "source": [
        "This table is pretty informative. Using what we learned from last week, we can say that for the 2020 election,\n",
        "\n",
        "* A 1% increase in the unemployment rate was associated with a 2.3% *decrease* in republican voteshare.\n",
        "* A 1000-person increase in population was associated with 0.029% decrease in republican voteshare.\n",
        "* both of these results are statistically signifiant at the 0.01 level.\n",
        "* 23% of the variation in republican voteshare can be explained by unemployment and population.\n",
        "\n",
        "Crucially, \"increase\" in this context pertains to *differences in between counties*!\n",
        "\n",
        "We can also compare these results across different elections. The coefficient for the unemployment variable in 2020 is over twice the size of the same coefficient in 2016! So it looks like actually unemployment and republican voteshare are *negatively* related, contrary to popular belief.\n",
        "\n",
        "But is this the whole story?\n",
        "\n",
        "Below, i've isolated West Virginia, one of the states with the highest unemployment rates in America. Instead of drawing a new regression line every year, i've drawn a **new regression line for each county over the six elections**.         "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wyvKJdpylbT"
      },
      "outputs": [],
      "source": [
        "west_virginia=df_c[(df_c['state']==54)]\n",
        "sns.lmplot(data=west_virginia, x='unemployment', y='r_votes', ci=None, hue='county', height=10, legend=False, palette='husl', scatter_kws={'alpha':.3})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3D0A8c3ylbT"
      },
      "source": [
        "Within a given county, **an increase in the unemployment** rate is associated with an **increase in republican voteshare**! This is where the second question comes in (variation within counties).\n",
        "\n",
        "We got away with doing a series of cross-sectional analyses (a new regression for each election) because we have over 3000 counties, so $n>3000$ for each of those regressions (though even so, we're still splitting our data up and it would be better to leverage the full dataset of >18000 observations in one regression). It also provides relatively useful information about the importance of unemployment across the country for each election. We can't really apply the same thinking to this situation, since we only have six time periods. If we ran a separate regression for each county, we would only have six observations per regression-- nowhere near enough to satisfy the central limit theorem (at least n>30). The insights would also be of limited utility; we would get over 3000 unique estimates for the realtionship between county-level employment and election results. Imagine trying to fit *that* into one table.\n",
        "\n",
        "Luckily, there's a way of modeling this relationship that allows us to account for differences in between counties, while also capturing the variation within counties. This is called a **Fixed Effect regression**\n",
        "\n",
        "> **Fixed Effects Models**: In experimental research, unmeasured differences between subjects are often controlled for via random assignment to treatment and control groups. Hence, even if a variable like Socio-Economic Status is not explicitly measured, because of random assignment, we can be reasonably confident that the effects of SES are approximately equal for all groups. Of course, random assignment is usually not possible with most survey research. If we want to control for the effect of a variable, we must explicitly measure it. If we don’t measure it, we can’t control for it. In practice, there will almost certainly be some variables we have failed to measure (or have measured poorly), so our models will likely suffer from some degree of **omitted variable bias**.\n",
        ">When we have **panel data** (the same people/states/counties. etc. measured at two or more points in time) another alternative presents itself: we can use the subjects as their own controls. With panel data we can **control for stable characteristics** (i.e. characteristics that do not change across time) whether they are measured or not. These include such things as **sex, race, and ethnicity for individuals**, or **urban/rural, topography, economic structure for geographic areas**. The idea is that, whatever effect these variables have at one point in time, they will have the **same effect** at a different point in time because the values of such variables do not change.\n",
        "\n",
        "A fixed effect regression takes the following form:\n",
        "\n",
        "$$\\huge Y_{it}=\\alpha_i+\\beta X_{it}+\\epsilon_{it}$$\n",
        "\n",
        "Where:\n",
        "* $X_{it}$ are the independent variables (e.g. population and unemployment) whose values vary over time.\n",
        "* $\\beta$ is the slope coefficient for variable $x$ (e.g. unemployment). The model assumes that these effects are time-invariant, e.g. the effect of $x$ is the same at time 1 as it is at time 4 (although the value of $x$ can be different at different time periods).\n",
        "* $\\alpha_i$ and $\\epsilon_{it}$ are both error terms. $\\epsilon_{it}$ is different for each individual at each point in time. $\\alpha_i$ only varies across individuals but not across time. We can think of $\\alpha_i$ as representing the effects of all the time invariant/stable variables that have NOT been included in the model. So, given that we have 6 time periods for each county then the six records for county 1 would all have the same value for $\\alpha_1$, the six records for county 2 would all have the same value for $\\alpha_2$, etc. But, $\\epsilon_{it}$ is free to be different for every case at every time period.\n",
        "\n",
        "A fixed effect regression allows us to account for $\\alpha_i$ through a technique called **demeaning**\n",
        "\n",
        ">**Demeaning**: After demeaning, all variables for all cases have a mean of 0. That means that all the between-subject variability has been eliminated. All that is left is the within-subject variability. So, with a fixed effects model, we are analyzing what causes individual’s values to change across time. Variables whose values do not change (like race or gender) cannot cause changes across time (unless their effects change across time as well). However, whatever effect they have at one time is the same effect that they have at other times, so the effects of such stable characteristics are controlled.\n",
        "\n",
        "In essence, you can picture this as allowing you to draw a separate regression line through each set of observations from the same group in your data (in this case, one county over time); however, while the *intercept* of these lines can vary (their absolute position), they will all have the same *slope* and will therefore be parallel. This is important, as we want to find one slope-- one common effect of x-- that fits *all* groups."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiaclcfkylbT"
      },
      "source": [
        "Run the command below to install the library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znZYGnJvylbT"
      },
      "outputs": [],
      "source": [
        "!pip install linearmodels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXKlp_dZylbU"
      },
      "outputs": [],
      "source": [
        "from linearmodels import PanelOLS\n",
        "from linearmodels import RandomEffects\n",
        "import statsmodels.formula.api as smf\n",
        "from linearmodels.panel import compare\n",
        "\n",
        "df_c=df_c.set_index(['county_fips','year']) # set the index to the county fips code and the year\n",
        "panel = PanelOLS.from_formula('r_votes ~ 1  + population + unemployment  + EntityEffects',df_c).fit() # run a fixed effects model\n",
        "print(compare({'Fixed Effects': panel,}, stars=True)) # print the model formatted as a regression table"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "R-squared (Within): 0.0206\n",
        "The model explains about 2% of the variation in Republican vote share within counties over time. Expected - vote share is very stable within counties from election to electoin.\n",
        "\n",
        "R-squared (Between): −0.1061\n",
        "This is negative because fixed effects do not attempt to explain variation between counties (that variation has been subtracted out).\n",
        "Ignore this value.\n",
        "\n",
        "R-squared (Overall): −0.0939\n",
        "This combines within and between variation and often becomes negative in FE models.\n",
        "Also ignore.\n",
        "\n"
      ],
      "metadata": {
        "id": "Qou9BvxJ3kiv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3AUDxWzylbU"
      },
      "source": [
        "When accounting for time-invariant differences between counties, the effect of population remains negative. This suggests that counties in which the population is *decreasing* tend to experience an increase in republican voteshare. More specifically, for every 1000 people that leave a county, republican voteshare increases by 0.06%.\n",
        "\n",
        "The really interesting part of this regression table, however, is the **coefficient on the unemployment variable**, which is now **positive**. This suggests that-- once we account for the differences between counties-- an increase in the unemployment rate *within* a county is *positively* associated with republican voteshare. Indeed, a 1% increase in the unemployment rate leads to a 0.28% increase in republican voteshare.\n",
        "\n",
        "This regression output even gives us three separate $R^2$ values-- one for between-variation, another for within, and one overall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWut2CBfylbU"
      },
      "source": [
        "---------------------------\n",
        "## 2. Difference in Differences\n",
        "\n",
        "One of the reasons that we observe a signficant relationship between unemployment and voting behaviour in last week's workshop is that the **Republican and Democratic parties have opposing views on what to do about unemployment**. Democratic lawmakers have historically been in favour of increasing the minimum wage to benefit low-income workers, while Republicans have generally opposed this on the basis that it would hurt these very workers by increase unemployment. Indeed, classical economic theory holds that an increase in wages would lead to a reduction in employment; A business that makes $100k in revenue per year and spends all of it on employing 20 people can't suddenly start paying their workers double their salaries-- unless it fires half of its workers. This is obviously a simplified model though-- minimum wage laws typically don't double wages, and businesses don't operate at-cost, they turn a profit which they could use to pay their workers more. In the rest of this workshop, we're going to be investigating this question empirically:\n",
        "\n",
        "### Do minimum wage laws increase unemployment?\n",
        "\n",
        "Note that this is a *causal* question; i'm not asking if they're correlated-- i'm asking if one causes the other. The burden of proof here is much higher than observing correlations, and we have to think seriously about **endogeneity**. In partiuclar, we need to account for the influence of **omitted variables** (e.g. a recession, or the economic composition of a state), the potential for **reverse causality** (states implementing minimum wage laws in response to unemployment crises), and **selection bias**.\n",
        "\n",
        "In a lab, you can **conduct causal inference** by running an experiment. You can randomly select individuals, split them into a control group and a treatment group, measure their values in an outcome variable prior to a treatment, administer a treatment, and measure their respective values after the treatment. If you observe a change in the outcome variable in the treatment group after having administered the treatment, you can interpert that as the causal effect of treatment. This is because we're able to make a plausible argument that the **control group can act as a counterfactual (a stand-in) for the treatment group in the absence of treatment**. Both groups had the same values before the treatment, then the only thing that changed between them was the treatment, so if we observe a change in the outcome variable, it must be due to treatment.\n",
        "\n",
        "In the real world, we rarely get to run expermients of this kind. Instead, we have to hunt for **natural experiments**: situations in which there is a **treatment** which we're interested in measuring the effect of, and two groups that can plausibly act as a treatment and control group.\n",
        "\n",
        "> **[Difference in Difference](https://www.publichealth.columbia.edu/research/population-health-methods/difference-difference-estimation#:~:text=DID%20relies%20on%20a%20less,individual%20level%20is%20not%20possible.)** is a quasi-experimental design that makes use of **longitudinal data** from **treatment** and **control** groups to obtain an appropriate **counterfactual** to estimate a causal effect. DID is typically used to **estimate the effect of a specific intervention or treatment** (such as a passage of law, enactment of policy, or large-scale program implementation) by comparing the changes in outcomes over time between a population that is enrolled in a program (the intervention group) and a population that is not (the control group).\n",
        "\n",
        "The Difference in Difference model can be estimated as a simple regression model of the following form:\n",
        "\n",
        "$$\\huge Y_{it} = \\beta_0 + \\beta_1 Treatment_i + \\beta_2 Post_t + \\beta_3 (Treatment_i \\times Post_t) + \\varepsilon_{it}$$\n",
        "\n",
        "- $Treatment_i$ is 0 for the control group and 1 for the treatment group\n",
        "- $Post_t$ is 0 for before and 1 for after\n",
        "\n",
        "we can insert the values of $Treatment$ and $Post$ using the table below and see that coefficient ($\\beta_3$) of the interaction of $Treatment$ and $Post$ is the Difference in Differences (DID) estimator:\n",
        "\n",
        "[Card and Krueger (1994)](https://davidcard.berkeley.edu/papers/njmin-aer.pdf) found one such natural experiment, allowing them to estimate the causal effect of an **increase in the state minimum wage on unemployment** using a DiD model; In 1992, New Jersey raised the state minimum wage from \\$4.25 to \\$5.05 while the minimum wage in neighbouring Pennsylvania stayed the same at \\$4.25.\n",
        "   \n",
        "* Treatmeng Group: New Jersey\n",
        "* Control Group: Pennsylvania\n",
        "* Pre-Treatment Period: before 1992\n",
        "* Post-Treatment Period: after 1992\n",
        "\n",
        "They conducted a survey of 384 fast-food restaurants across both states, right before and right after the law came into effect in New Jersey, asking them how many people they employed. They ran a Difference-in-Differences model, and found that the coefficient $\\beta_3$ was positive but not statistically significant. In other words, the average total employees per restaurant *increased* after the minimum wage increased, but this could have been due to random chance.\n",
        "\n",
        "That was a long time ago. Things have changed since then, including the fact that we have access to a lot more data and computational power. Let's see if we can replicate Card and Krueger's results with more recent data. I've downloaded data on unemployment, minimum wage levels, and Gross Domestic Product at the state level going back to 1976. Let's have a look at minimum wages in New Jersey and Pennsylvania over time:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ew9bmp8ylbU"
      },
      "outputs": [],
      "source": [
        "df_s=pd.read_csv('https://storage.googleapis.com/qm2/wk10/state_data.csv', parse_dates=['date']) # read in the state-level data\n",
        "did=df_s[df_s['state'].isin(['pennsylvania', 'new jersey'])] # subset the data to only include pennsylvania and new jersey\n",
        "\n",
        "px.line(did, x='date', y='minwage', color='state', title=\"Minimum Wages in New Jersey and Pennsylvania\") # plot the minimum wage over time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oaz3RYkVylbU"
      },
      "source": [
        "The plot above sort of looks like a set of descending staircases; this is for two reasons. The plateaus exist because each row in the dataframe `df_s` is the value of a state in a given *month*, but we only have minimum wage data for every *year*. So we get 12 consecutive values of minimum wage every year. The reason that the staircases are descending is because these minimum wages are adjusted for inflation. No matter where you're from, you've probably heard a grandparent say something along the lines of \"My parents would send me to the shops with 25 cents to buy groceries for the week\", but now it costs £9 for a bag of chips. That's inflation-- every year things tend to get slightly more expensive, so if the same *absolute* minimum wage actually diminishes in \"real\" terms, which is what the variable `minwage` measures. Incidentally, this is one of the main reasons University staff have been on [strike](https://www.ucu.org.uk/article/11830/University-staff-pay-cut-by-20-new-figures-show). Anyway. Back to minimum wages.\n",
        "\n",
        "This plot shows that **for the past fifty years**, New Jersey and Pennsylvania have had **largely similar minimum wage policies**. There have been a couple moments of divergence, including in the 1990s when the Card and Krueger study was conducted. However, the biggest divergence actually started taking place in **2014** when New Jersey seems to have begun taking a wildly different approach. While Pennsylvania has had the same minimum wage since 2008 (and therefore seen a decline in inflation-adjusted wages), **New Jersey has raised the minimum wage significantly twice**. In **2020**, New Jersey's minimum wage was around 50% higher than Pennsylvania's. We can **exploit** the fact that these two states have **historically had similar minimum wage laws** but have recently experienced a big divergence to see if that change in minimum wages has resulted in a change in employment levels.\n",
        "\n",
        "Our Difference-in-Differences setup is as follows:\n",
        "\n",
        "$$\\large Unemployment_{state, year} = \\beta_0 + \\beta_1 Treatment_{state} + \\beta_2 Post_{year} + \\beta_3 (Treatment_{state} \\times Post_{year}) + \\beta_4 GDP_{state,year} + \\varepsilon_{it}$$\n",
        "\n",
        "* New Jersey is the **treatment group** (1)\n",
        "* Pennsylvania is the **control group** (0)\n",
        "* Years before 2014 is the **pre-treatment period** (0)\n",
        "* Years after 2014 is the **post-treatment period** (1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GdVFn-WylbU"
      },
      "outputs": [],
      "source": [
        "did['post']=np.where(did['date']>='2014-01-01',1,0) # create a variable that is 1 if the date is after the minimum wage increase and 0 otherwise\n",
        "did['treatment']=np.where(did['state']=='new jersey',1,0) # create a variable that is 1 if the state is new jersey (i.e., the treatment group) and 0 for pennsylvania (the control group)\n",
        "did['post_treatment']=did['post']*did['treatment'] # create a variable that is 1 if the date is after the minimum wage increase and the state is new jersey and 0 otherwise"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VARIABLES\n",
        "\n",
        "post\n",
        "- 1 = data **after** the 2014 minimum wage increase\n",
        "- 0 = data **before** the 2014 minimwm wage increase\n",
        "\n",
        "treatment (before and after)\n",
        "\n",
        "- 1 = **New Jersey** (treatment)\n",
        "- 0 = **Pennsylvania** (control)\n",
        "\n",
        "post_treatment\n",
        "\n",
        "- 1 = data for **New Jersey** **after** the 2014 minimum wage increase\n",
        "- 0 = everything else\n"
      ],
      "metadata": {
        "id": "K_hiuejU8ZSl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_u8nrN2ylbU"
      },
      "source": [
        "Before we proceed with the analysis, though, we need to **satisfy two assumptions** that will allow us to argue that Pennsylvania can act as a valid control group for New Jersey:\n",
        "\n",
        "1. **No simultaneous treatments:**\n",
        "    * If, for example, New Jersey suddenly entered a massive recession in 2014 as well, we couldn't really argue that resulting effects on employment are due solely to the minimum wage law. To account for this, we'll be including **state-level GDP** as an **additional independent variable** in our DiD model.\n",
        "\n",
        "2. **Parallel Trends:**\n",
        "    * Both states have to have been experiencing **similar trends** in the **dependent variable** (unemployment) **prior to the treatment** (minimum wage law). If they were trending in opposite directions for unobserved reasons, ensuing differences in unemployment may be due to those unobserved reasons rather than the treatment.\n",
        "    * We can **check this** by **plotting the dependent variable for both groups over time**, and indicating the timing of the treatment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8one2vAvylbU"
      },
      "outputs": [],
      "source": [
        "did=did[(did['date']>='2008-01-01') & (did['date']<='2020-01-01')]\n",
        "sns.lineplot(data=did,x='date',y='unemployment',hue='state')\n",
        "plt.axvline(pd.to_datetime('2014-01-01'),color='black',linestyle='dashed', label='NJ Minimum Wage Increase')\n",
        "plt.title('Unemployment in Pennsylvania and New Jersey')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdC8Ek3nylbU"
      },
      "source": [
        "This plot shows a big **spike** in unemployment occurring for both Pennsylvania and New Jersey as a result of the **2008 financial crisis**. New jersey had a higher unemployment rate than Pennsylvania, but their trends are largely parallel and decreasing after 2012. In the years following the minimum wage law, New Jersey's unemployment rate actually dips below Pennsylvania's for the first time in years. Let's look at this in the form of boxplots:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BU5SkBv9ylbU"
      },
      "outputs": [],
      "source": [
        "did['category']=did['treatment'].astype(str)+did['post'].astype(str) # this variable is just for the plot below\n",
        "sns.boxplot(x='category', y='unemployment', hue='treatment', data=did).set_xticklabels([\"Pre x Treatment\", \"Pre x Control\",'Post x Treatment','Post x Control'])\n",
        "plt.xlabel('')\n",
        "plt.title('Unemployment Rates by Treatment and Post Treatment')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bks2VaeylbU"
      },
      "source": [
        "This plot is fascinating in and of itself. The two box plots on the **left** show the unemployment values of the counties **prior** to the minimum wage law in 2014, while the two on the **right** show their values **after** the minimum wage increases.  Pennsylvania (the \"control\" group) is colored in blue, and New Jersey (the \"treatment\" group) is colored orange. Prior to the minimum wage increase in 2014, Pennsylvania (blue) has a lower unemployment rate than New Jersey (orange). In the years following New Jersey's passage of the minimum wage law, New Jersey actually has a *lower* unemployment rate than Pennsylvania! This is the only boxplot where the \"treatment\" (a minimum wage law) is being applied, and it has the lowest unemployment rate.\n",
        "\n",
        "Let's see if this difference is **statistically signfiicant**, and calculate a treatment effect:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYnJCPamylbU"
      },
      "outputs": [],
      "source": [
        "did_model = ols('unemployment ~  post + treatment + post_treatment', did).fit()\n",
        "print(did_model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VARIABLES\n",
        "\n",
        "**post** = unemployment for both states after the 2014 minimum wage increase\n",
        "\n",
        "**treatment** = unemployment for New Jersey\n",
        "\n",
        "**post_treatment** = unemployment for New Jersey after the 2014 minimum wage increase"
      ],
      "metadata": {
        "id": "pbOGkepG9eUJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQLjTt71ylbU"
      },
      "source": [
        "There are some really interesting results from this model-- let's interpret the coefficients one by one.\n",
        "\n",
        "* `gdp`: GDP is inversely related to unemployment. This makes sense: GDP basically measures the total amount of economic activity, so more economic activity = more employment.\n",
        "* `post`: this coefficient is negative, but statistically insignificant at the 0.05 level; it indicates that unemployment *generally* decreased for both groups, but that this could be due to random chance.\n",
        "* `treatment`: again negative but insignficant, meaning that there is no significant difference in unemployment levels between NJ and PA over the entire period.\n",
        "* `post_treatment`: this is our difference-in-differences estimator, and reflects the causal effect of treatment. It is negative and statistically significant. If we believe that the asusmptions of our model are satisfied, we can claim that:\n",
        "    * **The introduction of a minimum wage in New Jersey led to a 1.95% decrease in unemployment relative to Pennsylvania**\n",
        "\n",
        "This is a bold claim. We should do our best to back it up. Notice that i've sort of arbitrarily chosen a window of dates around the minimum wage law-- maybe this result is a fluke, due to the timespan ive chosen.\n",
        "\n",
        "To address this concern, I'll **run the same model 10 times**, starting with a really small time window-- just one year on either side of the law-- and progressively expand it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHap1OiGylbV"
      },
      "outputs": [],
      "source": [
        "models=[] # create empty list to store the models\n",
        "names=[] # create empty list to store the names of the models\n",
        "\n",
        "for window in range(1,10): # loop through years from 2000 to 2020 in increments of 4\n",
        "    did=df_s[(df_s['date']>=str(2014-window)+'-01-01') & (df_s['date']<=str(2014+window)+'-01-01') & df_s['state'].isin(['pennsylvania', 'new jersey'])] # subset the data within the window of interest around 2014\n",
        "    did['post']=np.where(did['date']>='2014-01-01',1,0) # create a dummy variable indicating the period after the minimum wage increase\n",
        "    did['treatment']=np.where(did['state']=='new jersey',1,0) # create a dummy variable for treatment\n",
        "    did['post_treatment']=did['post']*did['treatment'] # create an interaction term between the post and treatment variables\n",
        "    did_model = ols('unemployment ~ gdp+ post + treatment + post_treatment', did).fit() # run the difference in difference model\n",
        "\n",
        "    models.append(did_model) # append the model to the list of models\n",
        "    names.append('± '+str(window)+' Year') # append the name of the model to the list of names\n",
        "\n",
        "table=summary_col( # create a regression table\n",
        "    models, # pass the models to the summary_col function\n",
        "    stars=True, # add stars denoting the p-values of the coefficient to the table; * p<0.05, ** p<0.01, *** p<0.001\n",
        "    float_format='%0.3f', # set the decimal places to 3\n",
        "    model_names=names, # set the names of the model\n",
        "    info_dict = {\"N\":lambda x: \"{0:d}\".format(int(x.nobs))}) # add the number of observations to the table\n",
        "\n",
        "print(table) # print the table\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZE3Y3oyQylbV"
      },
      "source": [
        "The row we're mainly interested in is the `post_treatment` coefficient, the **treatment effect**. It remains significant and negative in all time periods smaller than 8 years, after which point it becomes insignificant;\n",
        "\n",
        "How do you think this affects our conclusion?\n",
        "\n",
        " The 2014 minimum wage increase in New Jersey is associated with a statistically significant reduction in unemployment in the short-to-medium run (up to about 8 years), relative to Pennsylvania. When using very long time windows (8–10 years), the effect becomes statistically indistinguishable from zero, likely because long-run state differences dominate the data and the parallel trends assumption weakens.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assessed Question\n",
        "\n",
        "1. Find the state with the **largest increase in minimum wage in the 1990s**. This will be our **treatment** group.\n",
        "2. Set a **5 year window** on either side of the treatment date\n",
        "3. Make a **parallel trends plot** using **Alabama** as a control group\n",
        "4. Run a **difference in differences model**.\n",
        "\n",
        "\n",
        "What was the percentage change in unemployment that resulted from the introduction of a minimum wage in this case? Is the difference statistically significant?"
      ],
      "metadata": {
        "id": "LJ5VJZf493Tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_s.head()"
      ],
      "metadata": {
        "id": "zpNxhhE0AtFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Many of the rows have '0.0' in the minwage column, likely becuase they don't have any state-independent minimum wage laws, and therefore the minimum wage defaults to the federal minimum wage.\n",
        "\n",
        "----> make a new dataframe which has an 'effective minimum wage column', which contains the state minwge unless the value is '0.00', in which case it replaces the '0.00' with the fedminwage."
      ],
      "metadata": {
        "id": "gSXh7_kBH_hT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create new column which finds the effective minimum wage\n",
        "df_s['effective_minwage'] = df_s['minwage'].where(df_s['minwage'] != 0, df_s['fedminwage'])\n"
      ],
      "metadata": {
        "id": "TV591WFTKU5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#filter the dataframe for the years of interest (1990s)\n",
        "df_90s=df_s[(df_s['date'] >= '1990-01-01') & (df_s['date'] <= '1999-12-31')]\n",
        "df_90s.head()"
      ],
      "metadata": {
        "id": "_DpIfmo5BHeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minwage_summary = df_90s.groupby('state')['effective_minwage'].agg(['min', 'max']).reset_index()\n",
        "#find the biggest increase in minimum wage\n",
        "minwage_summary['increase'] = minwage_summary['max'] - minwage_summary['min']\n",
        "largest_increase_state = minwage_summary.loc[minwage_summary['increase'].idxmax()]\n",
        "\n",
        "print(\"State with the largest minimum wage increase in the 1990s:\")\n",
        "print(largest_increase_state)\n"
      ],
      "metadata": {
        "id": "kun3AbwL4lyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot minimum wage trends of alabama and idaho to visualise the increase in minwage\n",
        "did_assessedq = df_s[df_s['state'].isin(['idaho', 'alabama'])]\n",
        "px.line(did_assessedq, x='date', y='effective_minwage', color='state', title=\"Minimum Wage Trends (1986-1996) for Idaho and Alabama\") # plot the minimum wage over time"
      ],
      "metadata": {
        "id": "-t-LzNdTJbMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "period_of_interest = df_s[(df_s['state'].isin(['idaho', 'alabama'])) & (df_s['year'] >= 1986) & (df_s['year'] <= 1996)]\n",
        "px.line(period_of_interest, x='date', y='effective_minwage', color='state', title=\"Minimum Wage Trends (1985-1995) for Idaho and Alabama\") # plot the minimum wage over time"
      ],
      "metadata": {
        "id": "N9ggSrWmScb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our Difference-in-Differences setup is as follows:\n",
        "\n",
        "$$\\large Unemployment_{state, year} = \\beta_0 + \\beta_1 Treatment_{state} + \\beta_2 Post_{year} + \\beta_3 (Treatment_{state} \\times Post_{year}) + \\beta_4 GDP_{state,year} + \\varepsilon_{it}$$\n",
        "\n",
        "* Idaho is the **treatment group** (1)\n",
        "* Alabama is the **control group** (0)\n",
        "* Years before 1991 is the **pre-treatment period** (0)\n",
        "* Years after 1991 is the **post-treatment period** (1)"
      ],
      "metadata": {
        "id": "2Yo_Qp26TBsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # create a variable that is 1 if the date is after the minimum wage increase and 0 otherwise\n",
        "did_assessedq['post']=np.where(did_assessedq['date']>='1991-01-01',1,0)\n",
        "\n",
        "# create a variable that is 1 if the state is Michigan (treatment group) and 0 for Alabama (the control group)\n",
        "did_assessedq['treatment']=np.where(did_assessedq['state']=='idaho',1,0)\n",
        "\n",
        "# create a variable that is 1 if the date is AFTER the minwage increase AND the state is Michigan, 0 otherwise\n",
        "did_assessedq['post_treatment']=did_assessedq['post']*did_assessedq['treatment']"
      ],
      "metadata": {
        "id": "Y4BVayR2Rpiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CHECK FOR PARALLEL TRENDS"
      ],
      "metadata": {
        "id": "RX8MIpbLT5qZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ef6a051"
      },
      "source": [
        "**Reasoning**:\n",
        "To check the parallel trends assumption for the Difference-in-Differences model, I need to plot the unemployment rates for the treatment state (Idaho) and the control state (Alabama) over time, within the specified 5-year window around the treatment date (January 1, 1991). This plot will help visualize if the trends in unemployment were similar before the treatment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98b238d8"
      },
      "source": [
        "#plot unemployment trends of alabama and idaho to assess the parallel trends assumption\n",
        "\n",
        "parallel_trends=did_assessedq[(did_assessedq['date']>='1986-01-01') & (did_assessedq['date']<='1996-01-01')]\n",
        "sns.lineplot(data=parallel_trends,x='date',y='unemployment',hue='state')\n",
        "plt.axvline(pd.to_datetime('1991-01-01'),color='black',linestyle='dashed', label='Idaho Minimum Wage Increase')\n",
        "plt.title('Unemployment in Alabama and Idaho')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#look at this in box plots\n",
        "# Re-create did_assessedq with the correct window and states for self-containment\n",
        "# The treatment state is Washington, control is Alabama, treatment date is 1991-01-01, 5-year window\n",
        "did_assessedq = df_s[(df_s['state'].isin(['idaho', 'alabama'])) &\n",
        "                     (df_s['date'] >= '1986-01-01') &\n",
        "                     (df_s['date'] <= '1996-01-01')].copy()\n",
        "\n",
        "# Create post, treatment, and post_treatment variables based on corrected dates and states\n",
        "did_assessedq['post'] = np.where(did_assessedq['date'] >= '1991-01-01', 1, 0)\n",
        "did_assessedq['treatment'] = np.where(did_assessedq['state'] == 'idaho', 1, 0)\n",
        "did_assessedq['post_treatment'] = did_assessedq['post'] * did_assessedq['treatment']\n",
        "\n",
        "did_assessedq['category']=did_assessedq['treatment'].astype(str)+did_assessedq['post'].astype(str) # this variable is just for the plot below\n",
        "sns.boxplot(x='category', y='unemployment', hue='treatment', data=did_assessedq).set_xticklabels([\"Pre x Control\", \"Pre x Treatment\",'Post x Control','Post x Treatment'])\n",
        "plt.xlabel('')\n",
        "plt.title('Unemployment Rates by Treatment and Post Treatment')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "S9S9qqw6UoUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3525697"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the parallel trends plot has been generated, the next step is to run the Difference-in-Differences model using the prepared `did_assessed` DataFrame. This involves creating the necessary `post`, `treatment`, and `post_treatment` interaction variables and then fitting an OLS regression model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "did_model2 = ols('unemployment ~  post + treatment + post_treatment', did_assessedq).fit()\n",
        "print(did_model2.summary())"
      ],
      "metadata": {
        "id": "iqv65H5WYGLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "INTERPRETATION\n",
        "\n",
        "post_treatment: coefficient 0.2903 (1.254): idaho's unemployment levels increased by 0.29% relative to Alabama because of the minimum wage increase, this is statistically insignificant."
      ],
      "metadata": {
        "id": "t8VM6E8WdaeQ"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "assessed_question_W9. Distributions and Basic Statistics.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "claymodel",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}